---
title: "Learning Low Dimensional Metrics"
collection: publications
permalink: /publication/2022-02-07-low-dim-metrics
excerpt: 'This paper investigates the theoretical foundations of metric learning.'
date: 2017-12-01
venue: 'Neural Information Processing Systems'
paperurl: 'https://proceedings.neurips.cc/paper/2017/file/f12ee9734e1edf70ed02d9829018b3d9-Paper.pdf'
citation: '<b>Mason, B.</b>, Jain, L., & Nowak, R. (2017). Learning low-dimensional metrics. <i>Advances in neural information processing systems</i>, 30.'
---
This paper investigates the theoretical foundations of metric learning, focused on three key questions that are not fully addressed in prior work: 1) we consider learning general low-dimensional (low-rank) metrics as well as sparse metrics;2) we develop upper and lower (minimax) bounds on the generalization error; 3)we quantify the sample complexity of metric learning in terms of the dimension of the feature space and the dimension/rank of the underlying metric; 4) we also bound the accuracy of the learned metric relative to the underlying true generative metric. All the results involve novel mathematical approaches to the metric learning problem, and also shed new light on the special case of ordinal embedding (aka non-metric multidimensional scaling).

[Download paper here](https://proceedings.neurips.cc/paper/2017/file/f12ee9734e1edf70ed02d9829018b3d9-Paper.pdf)

Recommended citation: <b>Mason, B.</b>, Jain, L., & Nowak, R. (2017). Learning low-dimensional metrics. <i>Advances in neural information processing systems</i>, 30.